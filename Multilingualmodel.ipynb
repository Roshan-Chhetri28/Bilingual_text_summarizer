{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rosha\\code\\textsummarizer\\summarizerv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#use tensorboard\n",
    "import pandas as pd\n",
    "#from transformers import pipeline, set_seed\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from huggingface_hub import Repository\n",
    "# from tqdm import tqdm\n",
    "# nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepali = pd.read_csv(\"Datasets/combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formatted_article</th>\n",
       "      <th>nepali_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"आर्टिफिसियल इन्टेलिजेन्स (एआई) को क्षेत्रमा क...</td>\n",
       "      <td>नेपालको स्वामित्वमा रहेको आर्टिफिसियल इन्टेलिज...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"काठमाण्डाै – नेपाल आयल निगमले आगामी माघ १ गते...</td>\n",
       "      <td>नेपाल आयल निगमले विभिन्न डिपोमा पेट्रोल, डिजेल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"नेपाली पर्यटकका लागि सबैभन्दा आकर्षक बाह्य पर...</td>\n",
       "      <td>भारत, संयुक्त अरब इमिरेट्स (युएई) र थाइल्याण्ड...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"काठमाण्डाै – सुनचाँदीको मूल्य माथी उकालो लागे...</td>\n",
       "      <td>नेपाल सुनचाँदी व्यवसायी महासंघका अनुसार आज छाप...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"नेपाल विद्युत् प्राधिकरणले डेडिकेटेड फिडर र ट...</td>\n",
       "      <td>नेपाल विद्युत् प्राधिकरणले बारा–पर्सा औद्योगिक...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   formatted_article  \\\n",
       "0  \"आर्टिफिसियल इन्टेलिजेन्स (एआई) को क्षेत्रमा क...   \n",
       "1  \"काठमाण्डाै – नेपाल आयल निगमले आगामी माघ १ गते...   \n",
       "2  \"नेपाली पर्यटकका लागि सबैभन्दा आकर्षक बाह्य पर...   \n",
       "3  \"काठमाण्डाै – सुनचाँदीको मूल्य माथी उकालो लागे...   \n",
       "4  \"नेपाल विद्युत् प्राधिकरणले डेडिकेटेड फिडर र ट...   \n",
       "\n",
       "                                      nepali_summary  \n",
       "0  नेपालको स्वामित्वमा रहेको आर्टिफिसियल इन्टेलिज...  \n",
       "1  नेपाल आयल निगमले विभिन्न डिपोमा पेट्रोल, डिजेल...  \n",
       "2  भारत, संयुक्त अरब इमिरेट्स (युएई) र थाइल्याण्ड...  \n",
       "3  नेपाल सुनचाँदी व्यवसायी महासंघका अनुसार आज छाप...  \n",
       "4  नेपाल विद्युत् प्राधिकरणले बारा–पर्सा औद्योगिक...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepali.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5048, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nepali.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepali['dialogue'] = nepali.formatted_article\n",
    "nepali['summary'] = nepali.nepali_summary\n",
    "\n",
    "nepali.drop(columns= ['formatted_article','nepali_summary'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = pd.read_csv(\"Datasets/samsum-train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english.drop(columns ='id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14732, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNDERSAMPLING ENGLISH DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "english_sampled = english.sample(n=5048, random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_pd= pd.concat([nepali[:500], english_sampled[:500]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_pd=language_pd.sample(frac=1, random_state= 50).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dialogue', 'summary'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_pd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = Dataset.from_pandas(language_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dialogue', 'summary'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 58018.12 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dialogue', 'summary'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language =language.filter()\n",
    "language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\Rosha\\code\\textsummarizer\\summarizerv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rosha\\code\\textsummarizer\\summarizerv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoints = 'google/mT5-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [11566, 45251, 1649, 4332, 64418, 1048, 14161, 7163, 259, 60315, 1131, 15649, 48995, 57123, 259, 13576, 21799, 82039, 1649, 117327, 24230, 58208, 1131, 34196, 7741, 259, 79254, 3081, 2661, 3596, 2237, 259, 378, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = tokenizer(\"नेपाल आयल निगमले स्थानीय तहको निर्वाचनका कारण पेट्रोलियम पदार्थको मूल्य वृद्धि गरेको छ । \")\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁नेपाल',\n",
       " '▁आय',\n",
       " 'ल',\n",
       " '▁नि',\n",
       " 'गम',\n",
       " 'ले',\n",
       " '▁स्थान',\n",
       " 'ीय',\n",
       " '▁',\n",
       " 'तह',\n",
       " 'को',\n",
       " '▁निर्',\n",
       " 'वाच',\n",
       " 'नका',\n",
       " '▁',\n",
       " 'कारण',\n",
       " '▁पे',\n",
       " 'ट्रो',\n",
       " 'ल',\n",
       " 'ियम',\n",
       " '▁पद',\n",
       " 'ार्थ',\n",
       " 'को',\n",
       " '▁मूल',\n",
       " '्य',\n",
       " '▁',\n",
       " 'वृद्ध',\n",
       " 'ि',\n",
       " '▁गर',\n",
       " 'ेको',\n",
       " '▁छ',\n",
       " '▁',\n",
       " '।',\n",
       " '</s>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input = 1024\n",
    "max_target_length = 100\n",
    "\n",
    "def preprocess_function(txt):\n",
    "    model_inputs = tokenizer(txt['dialogue'], max_length=max_input,truncation=True)\n",
    "    labels = tokenizer(txt['summary'], max_length=max_target_length, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2263.72 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_language = language.map(preprocess_function, batched = True)\n",
    "tokenized_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FineTuning with Trainer API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_test = tokenized_language.train_test_split(test_size=0.2, seed=42)\n",
    "X_train = train_test['train']\n",
    "X_test = train_test['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 4\n",
    "num_train_epochs = 5\n",
    "\n",
    "logging_steps = 10 #len(X_train)//batch_size\n",
    "model_name = model_checkpoints.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\"bilingular_model\", eval_strategy=\"epoch\", learning_rate=5.6e-5, per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps, push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rouge_score\n",
    "def compute_metric(eval_pred):\n",
    "    predictions , labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from typing import Optional\n",
    "# class CSCSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    # def _save(self, output_dir: Optional[str] = None, state_dict=None):\n",
    "    #     if state_dict ==None:\n",
    "    #         state_dict = self.model.state_dict()\n",
    "        \n",
    "    #     for key, tensor in state_dict:\n",
    "    #         if isinstance(torch, torch.Tensor):\n",
    "    #             state_dict[key] = tensor.contiguous()\n",
    "        \n",
    "\n",
    "    #     super._save(output_dir, state_dict)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.trainer_seq2seq.Seq2SeqTrainer at 0x291921b8f50>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args= args,\n",
    "    train_dataset=X_train,\n",
    "    eval_dataset=X_test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_model_contiguous(model, save_directory):\n",
    "    for param in model.parameters():\n",
    "        if not param.is_contiguous():\n",
    "            param.data = param.data.contiguous()\n",
    "\n",
    "    model.save_pretrained(save_directory)\n",
    "\n",
    "save_model_contiguous(model, \"bilingual_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bilingual_model/bilingual_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"bilingual_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/MKCL/Bilingual-mode_Nepali_and_English into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "repo_name = \"MKCL/Bilingual-mode_Nepali_and_English\"\n",
    "repo = Repository(local_dir=\"modelli\", clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n",
      "remote: Password authentication in git is no longer supported. You must use a user access token or an SSH key instead. See https://huggingface.co/blog/password-git-deprecation\n",
      "fatal: Authentication failed for 'https://huggingface.co/MKCL/Bilingual-mode_Nepali_and_English/'\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "remote: Password authentication in git is no longer supported. You must use a user access token or an SSH key instead. See https://huggingface.co/blog/password-git-deprecation\nfatal: Authentication failed for 'https://huggingface.co/MKCL/Bilingual-mode_Nepali_and_English/'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Rosha\\code\\textsummarizer\\summarizerv\\Lib\\site-packages\\huggingface_hub\\repository.py:1117\u001b[0m, in \u001b[0;36mRepository.git_push\u001b[1;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m return_code:\n\u001b[1;32m-> 1117\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError(return_code, process\u001b[38;5;241m.\u001b[39margs, output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[1;31mCalledProcessError\u001b[0m: Command '['git', 'push', '--set-upstream', 'origin', 'main']' returned non-zero exit status 128.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rosha\\code\\textsummarizer\\summarizerv\\Lib\\site-packages\\huggingface_hub\\repository.py:1325\u001b[0m, in \u001b[0;36mRepository.push_to_hub\u001b[1;34m(self, commit_message, blocking, clean_ok, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_add(auto_lfs_track\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgit_commit(commit_message)\n\u001b[1;32m-> 1325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgit_push\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morigin \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_branch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_lfs_prune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rosha\\code\\textsummarizer\\summarizerv\\Lib\\site-packages\\huggingface_hub\\repository.py:1120\u001b[0m, in \u001b[0;36mRepository.git_push\u001b[1;34m(self, upstream, blocking, auto_lfs_prune)\u001b[0m\n\u001b[0;32m   1117\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError(return_code, process\u001b[38;5;241m.\u001b[39margs, output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(exc\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blocking:\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstatus_method\u001b[39m():\n",
      "\u001b[1;31mOSError\u001b[0m: remote: Password authentication in git is no longer supported. You must use a user access token or an SSH key instead. See https://huggingface.co/blog/password-git-deprecation\nfatal: Authentication failed for 'https://huggingface.co/MKCL/Bilingual-mode_Nepali_and_English/'\n"
     ]
    }
   ],
   "source": [
    "repo.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizerv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
